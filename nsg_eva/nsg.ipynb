{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neighbor:\n",
    "    def __init__(self, id, distance, flag):\n",
    "        self.id = id\n",
    "        self.distance = distance\n",
    "        self.flag = flag\n",
    "\n",
    "    def __lt__(self, other: 'Neighbor'):\n",
    "        return self.distance < other.distance\n",
    "    \n",
    "    def printNeighbor(self):\n",
    "        print(\"id: \", self.id, \"distance: \", self.distance, \"flag: \", self.flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSG Index structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(query_data, db_vec, dimension: int):\n",
    "    return np.sum((query_data - db_vec) ** 2)\n",
    "\n",
    "def print_retset(retset):\n",
    "    for i in range(len(retset)):\n",
    "        retset[i].printNeighbor()\n",
    "        \n",
    "class IndexNSG():\n",
    "    def __init__(self, dimension: int, n: int):\n",
    "        self.final_graph = []\n",
    "        self.dimension = dimension\n",
    "        self.width = 0\n",
    "        self.ep_ = 0\n",
    "        self.nd_ = n\n",
    "    \n",
    "    def Load(self, filename: str):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self.width = int.from_bytes(f.read(4), byteorder='little', signed=False)\n",
    "            self.ep_ = int.from_bytes(f.read(4), byteorder='little', signed=False)\n",
    "            cc = 0\n",
    "            while True:\n",
    "                k_bytes = f.read(4)\n",
    "                if not k_bytes:\n",
    "                    break\n",
    "                k = int.from_bytes(k_bytes, byteorder='little', signed=False)\n",
    "                cc += k\n",
    "                tmp = list(np.frombuffer(f.read(k * 4), dtype=np.uint32))\n",
    "                self.final_graph.append(tmp)\n",
    "            cc //= self.nd_\n",
    "            # print(cc)\n",
    "\n",
    "    def search_with_base_graph(self, query, x, K, parameters):\n",
    "        L = parameters['L_search']\n",
    "        data_ = x\n",
    "        retset:list[Neighbor] = []\n",
    "        init_ids = []\n",
    "        flags = [0] * self.nd_\n",
    "        \n",
    "        for tmp_l in range(min(L, len(self.final_graph[self.ep_]))):\n",
    "            init_ids.append(self.final_graph[self.ep_][tmp_l])\n",
    "            flags[init_ids[tmp_l]] = 1\n",
    "    \n",
    "        while tmp_l < L:\n",
    "            id = random.randint(0, self.nd_ - 1)\n",
    "            if flags[id] == 1:\n",
    "                continue\n",
    "            flags[id] = 1\n",
    "            init_ids.append(id)\n",
    "            tmp_l += 1\n",
    "\n",
    "        for i in range(len(init_ids)):\n",
    "            id = init_ids[i]\n",
    "            dist = compare(data_[id], query, self.dimension)\n",
    "            retset.append(Neighbor(id, dist, 1))\n",
    "        \n",
    "        retset.sort()\n",
    "        \n",
    "        # print(\"=====================================\")\n",
    "        # print_retset(retset)\n",
    "        # print(\"=====================================\")\n",
    "        \n",
    "        k = 0\n",
    "        while k < L:\n",
    "            nk = L\n",
    "            if retset[k].flag:\n",
    "                retset[k].flag = 0\n",
    "                n = retset[k].id\n",
    "        \n",
    "                for m in range(len(self.final_graph[n])):\n",
    "                    id = self.final_graph[n][m]\n",
    "                    if flags[id] == 1:\n",
    "                        continue\n",
    "                    flags[id] = 1\n",
    "                    dist = compare(query, data_[id], self.dimension)\n",
    "                    if dist >= retset[L - 1].distance:\n",
    "                        continue\n",
    "                    nn = Neighbor(id, dist, 1)\n",
    "                    heapq.heappush(retset, nn)\n",
    "                    # print_retset(retset)\n",
    "                    # print(\"=====================================\")\n",
    "                    r = retset.index(nn)\n",
    "                    if r < nk:\n",
    "                        nk = r\n",
    "                    # nk: the index of the smallest dist\n",
    "            if nk <= k:\n",
    "                k = nk\n",
    "            else:  \n",
    "                k += 1\n",
    "                \n",
    "        # print_retset(retset)\n",
    "        \n",
    "        indices = [0] * K\n",
    "        for i in range(K):\n",
    "            indices[i] = heapq.heappop(retset).id\n",
    "        \n",
    "        # print(indices)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .fvecs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_load:  (1000000, 128)\n",
      "query_load:  (10000, 128)\n",
      "gt_load:  (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        dim_bytes = file.read(4)  # Read 4 bytes for dimension\n",
    "        dim = int.from_bytes(dim_bytes, byteorder='little')  # Convert bytes to integer for dimension\n",
    "\n",
    "        file.seek(0, 2)  # Move the file pointer to the end\n",
    "        fsize = file.tell()  # Get the file size\n",
    "        num = fsize // ((dim + 1) * 4)  # Calculate the number of data points\n",
    "\n",
    "        file.seek(0)  # Move the file pointer back to the beginning\n",
    "        data = np.empty((num, dim), dtype=np.float32)  # Create an empty numpy array to store data\n",
    "\n",
    "        for i in range(num):\n",
    "            file.seek(4, 1)  # Move the file pointer forward by 4 bytes to skip index\n",
    "            data[i] = np.fromfile(file, dtype=np.float32, count=dim)  # Read dim number of float values\n",
    "\n",
    "    return data, num, dim\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "# Change to the path of your data\n",
    "data_load, points_num, dim = load_data(\"./sift/sift_base.fvecs\")\n",
    "query_load, query_num, query_dim = load_data(\"./sift/sift_query.fvecs\")\n",
    "assert(dim == query_dim)\n",
    "\n",
    "gt_load = ivecs_read(\"./sift/sift_groundtruth.ivecs\")\n",
    "\n",
    "print(\"data_load: \", np.shape(data_load))\n",
    "print(\"query_load: \", np.shape(query_load))\n",
    "print(\"gt_load: \", np.shape(gt_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = IndexNSG(dim, points_num)\n",
    "index.Load(\"./sift.nsg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9118\n"
     ]
    }
   ],
   "source": [
    "K = 100\n",
    "L = 100\n",
    "assert(L >= K)\n",
    "qsize = 100\n",
    "paras = {'L_search': L}\n",
    "# for i in range(query_num):\n",
    "# for i in range(1):\n",
    "#     index.search_with_base_graph(query_load[i], data_load, K, paras)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(qsize):\n",
    "    indices = index.search_with_base_graph(query_load[i], data_load, K, paras)\n",
    "    gt = gt_load[i]\n",
    "    g = set(gt)\n",
    "    total += len(gt)\n",
    "    \n",
    "    for item in indices:\n",
    "        if item in g:\n",
    "            correct += 1\n",
    "\n",
    "acc = 1.0 * correct / total\n",
    "print(\"acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare():\n",
    "    # TODO\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "\n",
    "# define but not used\n",
    "def search_with_opt_graph(query, K, parameters, indices):\n",
    "    L = parameters['L_search']\n",
    "    \n",
    "    retset:list[Neighbor] = []\n",
    "    init_ids = []\n",
    "    flags = [0] * nd_\n",
    "    \n",
    "    neighbors = opt_graph[node_size * ep_ + data_len:]\n",
    "    MaxM_ep = neighbors[0]\n",
    "    neighbors = neighbors[1:]\n",
    "    \n",
    "    for tmp_l in range(min(L, MaxM_ep)):\n",
    "        init_ids.append(neighbors[tmp_l])\n",
    "        flags[init_ids[tmp_l]] = 1\n",
    "    \n",
    "    while tmp_l < L:\n",
    "        id = random.randint(0, nd_ - 1)\n",
    "        if flags[id] == 1:\n",
    "            continue\n",
    "        flags[id] = 1\n",
    "        init_ids.append(id)\n",
    "        tmp_l += 1\n",
    "    \n",
    "    L = 0\n",
    "    for i in range(len(init_ids)):\n",
    "        id = init_ids[i]\n",
    "        if id >= nd_:\n",
    "            continue\n",
    "        x = opt_graph[node_size * id:]\n",
    "        norm_x = x[0]\n",
    "        x = x[1:]\n",
    "        dist = compare(x, query, norm_x, dimension_)\n",
    "        retset.append(Neighbor(id, dist, 1))\n",
    "        flags[id] = 1\n",
    "        L += 1\n",
    "    \n",
    "    retset.sort()\n",
    "    k = 0\n",
    "    while k < L:\n",
    "        nk = L\n",
    "        \n",
    "        if retset[k].flag:\n",
    "            retset[k].flag = 0\n",
    "            n = retset[k].id\n",
    "            neighbors = opt_graph[node_size * n + data_len:]\n",
    "            MaxM = neighbors[0]\n",
    "            neighbors = neighbors[1:]\n",
    "            for m in range(MaxM):\n",
    "                id = neighbors[m]\n",
    "                if flags[id] == 1:\n",
    "                    continue\n",
    "                data = opt_graph[node_size * id:]\n",
    "                norm = data[0]\n",
    "                data = data[1:]\n",
    "                dist = compare(query, data, norm, dimension_)\n",
    "                if dist >= retset[L - 1].distance:\n",
    "                    continue\n",
    "                nn = Neighbor(id, dist, 1)\n",
    "                r = InsertIntoPool(retset.data(), L, nn)\n",
    "                if r < nk:\n",
    "                    nk = r\n",
    "        \n",
    "        if nk <= k:\n",
    "            k = nk\n",
    "        else:  \n",
    "            k += 1\n",
    "    \n",
    "    for i in range(K):\n",
    "        indices[i] = retset[i].id\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4119"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
